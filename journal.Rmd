---
title: "Journal (reproducible report)"
author: "Khawla Taleb Bouhemady"
date: "2021-01-08"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```



# Setting up the environment and Basic R coding

Last compiled: `r Sys.Date()`

### Chapter 1 : Done

```{r}
calc_EOQ <- function(D = 1000) {
  K <- 5
  h <- 0.25
  Q <- sqrt(2*D*K/h)
  Q
}

calc_EOQ()
```
# First Assignment : Bike Sales

```{r plot, fig.width=14, fig.height=7}
#importing packages
library(tidyverse)
library(readxl)
library(lubridate)
library("writexl")
#reading files
bikes_tbl <- read_excel(path = "DS_101/DS_101/00_data/01_bike_sales/01_raw_data/bikes.xlsx")
orderlines_tbl <- read_excel("DS_101/DS_101/00_data/01_bike_sales/01_raw_data/orderlines.xlsx")
bikeshops_tbl  <- read_excel("DS_101/DS_101/00_data/01_bike_sales/01_raw_data/bikeshops.xlsx")
#joining the tabels
bike_orderlines_joined_tbl <- orderlines_tbl %>%
  left_join(bikes_tbl, by = c("product.id" = "bike.id")) %>%
  left_join(bikeshops_tbl, by = c("customer.id" = "bikeshop.id"))
glimpse(bike_orderlines_joined_tbl)
# separating the location to state and city and calculating total price

bike_orderlines_wrangled_tbl <- bike_orderlines_joined_tbl %>%
  separate(col    = location,
           into   = c("city", "state"),
           sep    = ", ") %>%
  mutate(total.price = price * quantity) %>%
  select(-...1, -gender) %>%
  select(-ends_with(".id")) %>%
  bind_cols(bike_orderlines_joined_tbl %>% select(order.id)) %>%
  select(order.id, contains("order"), contains("model"), contains("location"),
         price, quantity, total.price,
         everything()) %>%
  set_names(names(.) %>% str_replace_all("\\.", "_"))

glimpse(bike_orderlines_wrangled_tbl)

#sales by state  - Bar plot

sales_by_state_tbl <- bike_orderlines_wrangled_tbl %>%
  select(state, total_price) %>%
  group_by(state) %>% 
  summarize(sales = sum(total_price)) %>%
  mutate(sales_text = scales::dollar(sales, big.mark = ".", 
                                     decimal.mark = ",", 
                                     prefix = "", 
                                     suffix = " €"))
sales_by_state_tbl

# Step 2 - Visualize

sales_by_state_tbl %>%
  ggplot(aes(x = state, y = sales)) +
  geom_col(fill = "#2DC6D6") + # Use geom_col for a bar plot
  geom_label(aes(label = sales_text)) + # Adding labels to the bars
  geom_smooth(method = "lm", se = FALSE) + # Adding a trendline
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
  labs(
    title    = "Revenue by state",
    subtitle = "Upward Trend",
    x = "", # Override defaults for x and y
    y = "Revenue"
  )+ theme(axis.text.x = element_text(angle = 45, hjust = 1))
# sales by year and location - 12 plots

sales_by_year_state_tbl <- bike_orderlines_wrangled_tbl %>%
  
  # Select columns and add a year
  select(order_date, total_price, state) %>%
  mutate(year = year(order_date)) %>%
  
  # Group by and summarize year and main catgegory
  group_by(year, state) %>%
  summarise(sales = sum(total_price)) %>%
  ungroup() %>%
  
  # Format $ Text
  mutate(sales_text = scales::dollar(sales, big.mark = ".", 
                                     decimal.mark = ",", 
                                     prefix = "", 
                                     suffix = " €"))

sales_by_year_state_tbl 



# Step 2 - Visualize
sales_by_year_state_tbl %>%
  
  # Set up x, y, fill
  ggplot(aes(x = year, y = sales, fill = state)) +
  
  # Geometries
  geom_col() + # Run up to here to get a stacked bar plot
  
  # Facet
  facet_wrap(~ state) +
  
  # Formatting
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
  labs(
    title = "Revenue by year and state",
    fill = "State" # Changes the legend name
  )


```

# Second Assignment : API & WEB Scraping

## API Challenge 
### This API to find out when the ISS (International Space Station) will be passing over Hamburg (which is at latitude 53.5511, longitude: 9.9937):
This API returns times to us in the form of Unix time.

```{r}
library(glue)
library(httr)
library(jsonlite)
resp <- GET("http://api.open-notify.org/iss-pass.json", query = list(lat =53.5511, lon = 9.9937))
resp
data = fromJSON(rawToChar(resp$content))
data

```
## WEB Scraping Challenge

```{r}
library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing
url_home          <- "https://www.rosebikes.com/"
#xopen(url_home) # Open links directly from RStudio to inspect them
# Read in the HTML for the entire webpage
html_home         <- read_html(url_home)
bike_family_tbl <- html_home %>%
                 # Get the nodes for the families ...
                 html_nodes(css=".main-navigation-category-with-tiles__title ")%>%
                 # ...and extract the information of the id attribute
                 html_text()%>%
  
                 #discard(.p = ~stringr::str_detect(.x,"Kids|Sale")) %>%
                 enframe(name = "position", value = "family_class") %>%
                 # Add a hashtag so we can get nodes of the categories by id (#)
                 mutate(
                     family_id = str_glue("#{family_class}")
                 )
bike_family_tbl
family_id_css <- bike_family_tbl %>%
                    pull(family_class) %>%
                    stringr::str_c(collapse = ", ")
family_id_css
# Extract the urls from the href attribute
bike_categories_tbl <- html_home %>%
           # Select nodes by the ids
          # html_nodes(css = (family_id_css) )%>%
           # Going further down the tree and select nodes by class
           # Selecting two classes makes it specific enough
           html_nodes( css=".catalog-category-title-with-video__title , .catalog-breadcrumb__list-item-link span , .catalog-navigation__link ") %>%
           html_attr("href")
           # Convert vector to tibble
           #enframe(name = "position", value = "subdirectory") %>%
           # Add the domain, because we will get only the subdirectories
          #mutate(url =glue("https://www.rosebikes.com{subdirectory}"))%>%
           # Some categories are listed multiple times.
           # We only need unique values
          # distinct(url)
bike_categories_tbl

```
# Third Assignment : Data Wrangling

```{r, eval=FALSE}

#Patents analysis ----
# Importing data: ---- 
library(vroom)
# Tidyverse
library(tidyverse)
# Data Table
library(data.table)
# Counter
library(tictoc)
# 2.0 DATA IMPORT ----
# Patents: ----
col_types <- list(
  id = col_character(),
  date = col_date("%Y-%m-%d"),
  num_claims = col_double()
)
patent_tbl <- vroom(
  file       = "patent.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)
#Assignee_id = id,
# Assignee: ----
col_types_assignee <- list(
  id = col_character(),
  type = col_character(),
  organization = col_character()
)
assignee_tbl <- vroom(
  file       = "assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types_assignee,
  na         = c("", "NA", "NULL")
)
# Patent assignee: ----
col_types_patent_assignee <- list(
  patent_id = col_character(),
  assignee_id = col_character()
)
patent_assignee_tbl <- vroom(
  file       = "patent_assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types_patent_assignee,
  na         = c("", "NA", "NULL")
)
col_types_uspc <- list(
  patent_id = col_character(),
  mainclass_id = col_number(),
  sequence = col_number()
)
uspc_tbl <- vroom(
  file       = "uspc.tsv", 
  delim      = "\t", 
  col_types  = col_types_uspc,
  na         = c("", "NA", "NULL")
)
# 3.1 Acquisition Data ----
setDT(assignee_tbl)
setDT(patent_tbl)
setDT(patent_assignee_tbl)
setDT(uspc_tbl)
patent_tbl %>% glimpse()
assignee_tbl %>% glimpse()
patent_assignee_tbl %>% glimpse()
uspc_tbl %>% glimpse()
# 4.0 DATA WRANGLING ----
# Target type = 2
# Start the analysis ----
#########################################################################
# Q1.Patent Dominance: What US company / corporation has the most patents? 
# List the 10 US companies with the most assigned/granted patents.
## Output: 
#########################################################################
# 4.1 summarize and count:
setnames(assignee_tbl, "id", "assignee_id")
combined_data <- merge(x = patent_assignee_tbl, y = assignee_tbl, by = "assignee_id")
us_patents <- combined_data %>%
  filter(type == 2)%>%
  filter(!is.na(patent_id) || !is.na(organization)) %>%
  select(-type, -assignee_id)%>% 
  group_by(organization) %>%
  count(patent_id) %>%
  select(-patent_id)%>%
  summarise(total = sum(n))%>%
  arrange(desc(total))   
us_top_10 <- us_patents %>% slice(1:10)

us_top_10_rds = "us_top_10.rds"
write_rds(us_top_10, us_top_10_rds)

#########################################################################
# Q2. Recent patent acitivity: What US company had the most patents granted in 2019? 
#List the top 10 companies with the most new granted patents for 2019.
#########################################################################
tbl_2 <- patent_tbl %>%   
         separate(col  = date,
         into = c("year", "month", "day"),
          sep  = "-", remove = TRUE) %>%
          mutate(
              month = as.numeric(month)
            )%>%
          filter(month == 01)%>%
          select(-year, -day)
#setnames(tbl_2, "id", "patent_id")
combined_data_2 <- merge(x = tbl_2, y = combined_data, by = "patent_id")
us_top10_2014_01 <- combined_data_2%>%
                    filter(country == "US")%>%
                    filter(!is.na(patent_id) || !is.na(organization)) %>%
                    select(organization, patent_id) %>%
                    group_by(organization) %>%
                    count(patent_id) %>%   
                    summarise(total_patents = sum(n))%>%
                    arrange(desc(total_patents)) %>% slice(1:10)  
us_top10_2014_01_new <- combined_data_2%>%
                        filter(country == "US" & num_claims == 1)%>%
                        filter(!is.na(patent_id) || !is.na(organization)) %>%
                        select(organization, patent_id) %>%
                        group_by(organization) %>%
                        count(patent_id) %>%   
                        summarise(total_patents = sum(n))%>%
                        arrange(desc(total_patents)) %>% slice(1:10)

us_top10_2014_01_new_rds = "us_top10_2014_01_new.rds"
write_rds(us_top10_2014_01_new, us_top10_2014_01_new_rds)
                  
 #########################################################################
# Q. Innovation in Tech: What is the most innovative tech sector? 
# What is the most innovative tech sector? For the top 10 companies (worldwide)
# with the most patents, what are the top 5 USPTO tech main classes?
#########################################################################
combined_data_3 <- merge(x = uspc_tbl, y = combined_data_2, by = "patent_id")
top10_worldwide_patents <- combined_data_3  %>%
                  filter(!is.na(patent_id) || !is.na(organization))%>%
                  group_by(organization) %>%
                  arrange(desc(mainclass_id)) %>% # set mainclass order first, the result will be sorted automatically 
                  count(patent_id) %>%
                  select(-patent_id)%>%
                  summarise(total_patents_worldwide = sum(n))%>%
                  ungroup() %>%
                  arrange(desc(total_patents_worldwide)) %>% slice(1:10)  
top10_worldwid_top5_upts_ <- top10_worldwide_patents %>% slice(1:5)  

top10_worldwid_top5_upts__rds = "top10_worldwid_top5_upts_.rds"
write_rds(top10_worldwid_top5_upts_, top10_worldwid_top5_upts__rds)



```

Q1.Patent Dominance: What US company / corporation has the most patents? 
List the 10 US companies with the most assigned/granted patents.
```{r}
read_rds("us_top_10.rds")
```

Q2. Recent patent acitivity: What US company had the most patents granted in 2019? 
List the top 10 companies with the most new granted patents for 2019
```{r}
read_rds("us_top10_2014_01_new.rds")
```

Q3. Innovation in Tech: What is the most innovative tech sector? 
What is the most innovative tech sector? For the top 10 companies (worldwide)
with the most patents, what are the top 5 USPTO tech main classes?

```{r}
read_rds("top10_worldwid_top5_upts_.rds")

```


# Fourth Assignment : Data Visualization

### Map the time course of the cumulative Covid-19 cases

```{r, echo = TRUE}
library(tidyverse)
library(dplyr)
library(lubridate)
library(ggplot2)
library(scales)
covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")
covid_data_tbl <- covid_data_tbl[order(as.Date(covid_data_tbl$dateRep, format="%d/%m/%Y")),]

covid_data_tbl2 <- covid_data_tbl %>%
  filter(countriesAndTerritories %in% c('Spain', 'United_Kingdom', 'France', 'Germany','United_States_of_America')) %>%
  select(dateRep, countriesAndTerritories, cases_weekly) %>%
  group_by(countriesAndTerritories) %>%
  mutate(cumulativeCases = cumsum(cases_weekly))  %>%
  select(dateRep, countriesAndTerritories, cumulativeCases) %>%
  rename(countries = countriesAndTerritories)
# Plotting the values 
ticks = c("Dec","Jan", 'Feb','March', 'April', 'May', 'June','July',
          'Aug','Sept','Oct','Nov','Dec')
y_ticks = seq(0,max(covid_data_tbl2$cumulativeCases),1250000)
covid_data_tbl2 %>%
  ggplot(aes(x = as.POSIXct(dateRep, format = '%d/%m/%Y'), y = cumulativeCases)) +
  geom_line(aes(color = countries), size = 1) +
  labs(x = 'Year 2020', y='Cumulative Cases', fill = 'Countries') +
  scale_x_datetime(date_breaks = 'month', labels = label_date_short()) +
  scale_y_continuous(breaks = c(y_ticks))
```
### Visualize the distribution of the mortality rate (deaths / population)

```{r, echo = TRUE}
library(tidyverse)
library(dplyr)
library(lubridate)
library(ggplot2)
theme_set(
  theme_dark()
)
covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")

world <- map_data('world') %>%
  rename(countries = region) %>%
  dplyr::select(countries,long,lat,group) 
  
covid_data_tbl <- covid_data_tbl %>%
  mutate(across(countriesAndTerritories, str_replace_all, "_", " ")) %>%
  mutate(countriesAndTerritories = case_when(
    
    countriesAndTerritories == "United Kingdom" ~ "UK",
    countriesAndTerritories == "United States of America" ~ "USA",
    countriesAndTerritories == "Czechia" ~ "Czech Republic",
    TRUE ~ countriesAndTerritories
    
  ))
population <- covid_data_tbl %>%
  group_by(countriesAndTerritories) %>%
  dplyr::select(countriesAndTerritories, popData2019) %>%
  unique() %>%
  rename(countries = countriesAndTerritories)
  
mortality_rate_tbl <- covid_data_tbl %>%
  group_by(countriesAndTerritories) %>%
  summarise( 
    total_deaths = sum(deaths_weekly)
    ) %>%
  rename(countries = countriesAndTerritories)
useful_map <- left_join(population,mortality_rate_tbl, by = "countries")
final_tbl <- left_join(world, useful_map, by = 'countries') %>%
  mutate(mort_rate = total_deaths / popData2019)
#plotting the values
ggplot(final_tbl, aes(long, lat, group = group))+
  geom_polygon(aes(fill = mort_rate), color = "white")+
  scale_fill_gradient(low = 'orange', high = 'red', na.value = 'white')

```


